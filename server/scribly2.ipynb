{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # to suppress some matplotlib deprecation warnings\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style as style\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "\n",
    "import ast\n",
    "import cv2\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import tqdm\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms, utils\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "from collections import OrderedDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Map the categories to a ist of classes\n",
    "cats = './categories.txt'\n",
    "\n",
    "# Load the categories\n",
    "categories = []\n",
    "with open(cats) as f:\n",
    "    categories = f.read().splitlines()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "def squeeze_weights(m):\n",
    "        m.weight.data = m.weight.data.sum(dim=1)[:,None]\n",
    "        m.in_channels = 1\n",
    "        \n",
    "model.conv1.apply(squeeze_weights);\n",
    "\n",
    "# Number of classes in the dataset\n",
    "num_classes = 340\n",
    "# Bias is set to True since we arent using BatchNorm, default in tensorflow\n",
    "model.fc = nn.Linear(in_features=512, out_features=num_classes, bias=True)\n",
    "\n",
    "\n",
    "checkpoint = torch.load(\"./checkpoint_resnet18.pth\", map_location=device) \n",
    "\n",
    "model.load_state_dict(checkpoint)\n",
    "\n",
    "\n",
    "# This applies softmax to the output of the model going to a normalized \"percentage\" of each class\n",
    "model.fc = nn.Sequential(OrderedDict([\n",
    "          ('fc', model.fc),\n",
    "          ('softmax', nn.Softmax())\n",
    "        ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample(x, y, spacing=1.0):\n",
    "    output = []\n",
    "    n = len(x)\n",
    "    px = x[0]\n",
    "    py = y[0]\n",
    "    cumlen = 0\n",
    "    pcumlen = 0\n",
    "    offset = 0\n",
    "    for i in range(1, n):\n",
    "        cx = x[i]\n",
    "        cy = y[i]\n",
    "        dx = cx - px\n",
    "        dy = cy - py\n",
    "        curlen = math.sqrt(dx*dx + dy*dy)\n",
    "        cumlen += curlen\n",
    "        while offset < cumlen:\n",
    "            t = (offset - pcumlen) / curlen\n",
    "            invt = 1 - t\n",
    "            tx = px * invt + cx * t\n",
    "            ty = py * invt + cy * t\n",
    "            output.append((tx, ty))\n",
    "            offset += spacing\n",
    "        pcumlen = cumlen\n",
    "        px = cx\n",
    "        py = cy\n",
    "    output.append((x[-1], y[-1]))\n",
    "    return output\n",
    "\n",
    "def normalize_resample_simplify(strokes, epsilon=1.0, resample_spacing=1.0):\n",
    "    if len(strokes) == 0:\n",
    "        raise ValueError('empty image')\n",
    "\n",
    "    # find min and max\n",
    "    amin = None\n",
    "    amax = None\n",
    "    for x, y in strokes:\n",
    "        cur_min = [np.min(x), np.min(y)]\n",
    "        cur_max = [np.max(x), np.max(y)]\n",
    "        amin = cur_min if amin is None else np.min([amin, cur_min], axis=0)\n",
    "        amax = cur_max if amax is None else np.max([amax, cur_max], axis=0)\n",
    "\n",
    "    # drop any drawings that are linear along one axis\n",
    "    arange = np.array(amax) - np.array(amin)\n",
    "    if np.min(arange) == 0:\n",
    "        raise ValueError('bad range of values')\n",
    "\n",
    "    arange = np.max(arange)\n",
    "    output = []\n",
    "    for x, y  in strokes:\n",
    "        xy = np.array([x, y], dtype=float).T\n",
    "        xy -= amin\n",
    "        xy *= 255.\n",
    "        xy /= arange\n",
    "        resampled = resample(xy[:, 0], xy[:, 1], resample_spacing)\n",
    "        # Simplification using the Ramer–Douglas–Peucker algorithm as recommended in the quickdraw github\n",
    "        simplified = simplify_coords(resampled, epsilon)\n",
    "        xy = np.around(simplified).astype(np.uint8)\n",
    "        output.append(xy.T.tolist())\n",
    "\n",
    "    return output\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    \"\"\"Doodles csv dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, nrows=1000, skiprows=None, size=256, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            nrows (int): Number of rows of file to read. Useful for reading pieces of large files.\n",
    "            skiprows (list-like or integer or callable): \n",
    "                    Line numbers to skip (0-indexed) or number of lines to skip (int) at the start of the file.\n",
    "            size (int): Size of output image.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        file = os.path.join(self.root_dir, csv_file)\n",
    "        self.size = size\n",
    "        self.doodle = pd.read_csv(file, usecols=['drawing', 'key_id', 'votes'], nrows=nrows, skiprows=skiprows)\n",
    "        self.transform = transform\n",
    "\n",
    "    @staticmethod\n",
    "    def _draw(raw_strokes,  size=256, lw=6, time_color=True):\n",
    "        BASE_SIZE = 256\n",
    "        img = np.zeros((BASE_SIZE, BASE_SIZE), np.uint8)\n",
    "        # Here is where the process diverges\n",
    "        # raw strokes is all strokes in this image so scale all accordingly\n",
    "\n",
    "        # remove the third dimension\n",
    "        raw_strokes = [stroke[0:2] for stroke in raw_strokes]\n",
    "\n",
    "        return normalize_resample_simplify(raw_strokes)\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.doodle)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # The ast traversal is important because it is a string being passed in \n",
    "        raw_strokes = ast.literal_eval(self.doodle.drawing[idx])\n",
    "        \n",
    "        label = self.doodle.key_id[idx].astype(np.int64)\n",
    "        votes = self.doodle.votes[idx].astype(np.float32)\n",
    "        sample = self._draw(raw_strokes, size=self.size, lw=2, time_color=True)\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return (sample[None]/255).astype('float32'), label, votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 104\n"
     ]
    }
   ],
   "source": [
    "SIZE = 224 # for matching to imagenet\n",
    "\n",
    "# select_nrows = 0\n",
    "doodles = CustomDataset(\"results.csv\", \"D:/scribbly/server/\", size=SIZE)\n",
    "\n",
    "print('Train set:', len(doodles))\n",
    "loader = DataLoader(doodles, batch_size=9, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "zero-size array to reduction operation minimum which has no identity",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m   plt\u001b[39m.\u001b[39mimshow(np\u001b[39m.\u001b[39mtranspose(img, (\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m)))\n\u001b[0;32m      6\u001b[0m plt\u001b[39m.\u001b[39mfigure(figsize\u001b[39m=\u001b[39m(\u001b[39m10\u001b[39m, \u001b[39m10\u001b[39m))\n\u001b[1;32m----> 7\u001b[0m \u001b[39mfor\u001b[39;00m  images, label, votes \u001b[39min\u001b[39;00m \u001b[39miter\u001b[39m(loader):\n\u001b[0;32m      8\u001b[0m   images \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39mnumpy()\n\u001b[0;32m      9\u001b[0m   label \u001b[39m=\u001b[39m label\u001b[39m.\u001b[39mnumpy()\n",
      "File \u001b[1;32mc:\\Users\\anhad\\miniconda3\\envs\\ml\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\anhad\\miniconda3\\envs\\ml\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\anhad\\miniconda3\\envs\\ml\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\anhad\\miniconda3\\envs\\ml\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[7], line 104\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    102\u001b[0m label \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdoodle\u001b[39m.\u001b[39mkey_id[idx]\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mint64)\n\u001b[0;32m    103\u001b[0m votes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdoodle\u001b[39m.\u001b[39mvotes[idx]\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mfloat32)\n\u001b[1;32m--> 104\u001b[0m sample \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_draw(raw_strokes, size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msize, lw\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m, time_color\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m    105\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform:\n\u001b[0;32m    106\u001b[0m     sample \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform(sample)\n",
      "Cell \u001b[1;32mIn[7], line 92\u001b[0m, in \u001b[0;36mCustomDataset._draw\u001b[1;34m(raw_strokes, size, lw, time_color)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[39m# Here is where the process diverges\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[39m# raw strokes is all strokes in this image so scale all accordingly\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \n\u001b[0;32m     89\u001b[0m \u001b[39m# remove the third dimension\u001b[39;00m\n\u001b[0;32m     90\u001b[0m raw_strokes \u001b[39m=\u001b[39m [stroke[\u001b[39m0\u001b[39m:\u001b[39m2\u001b[39m] \u001b[39mfor\u001b[39;00m stroke \u001b[39min\u001b[39;00m raw_strokes]\n\u001b[1;32m---> 92\u001b[0m \u001b[39mreturn\u001b[39;00m normalize_resample_simplify(raw_strokes)\n",
      "Cell \u001b[1;32mIn[7], line 37\u001b[0m, in \u001b[0;36mnormalize_resample_simplify\u001b[1;34m(strokes, epsilon, resample_spacing)\u001b[0m\n\u001b[0;32m     35\u001b[0m amax \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[39mfor\u001b[39;00m x, y \u001b[39min\u001b[39;00m strokes:\n\u001b[1;32m---> 37\u001b[0m     cur_min \u001b[39m=\u001b[39m [np\u001b[39m.\u001b[39;49mmin(x), np\u001b[39m.\u001b[39mmin(y)]\n\u001b[0;32m     38\u001b[0m     cur_max \u001b[39m=\u001b[39m [np\u001b[39m.\u001b[39mmax(x), np\u001b[39m.\u001b[39mmax(y)]\n\u001b[0;32m     39\u001b[0m     amin \u001b[39m=\u001b[39m cur_min \u001b[39mif\u001b[39;00m amin \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m np\u001b[39m.\u001b[39mmin([amin, cur_min], axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36mamin\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\anhad\\miniconda3\\envs\\ml\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2858\u001b[0m, in \u001b[0;36mamin\u001b[1;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m   2742\u001b[0m \u001b[39m@array_function_dispatch\u001b[39m(_amin_dispatcher)\n\u001b[0;32m   2743\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mamin\u001b[39m(a, axis\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, out\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, keepdims\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39m_NoValue, initial\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39m_NoValue,\n\u001b[0;32m   2744\u001b[0m          where\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39m_NoValue):\n\u001b[0;32m   2745\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   2746\u001b[0m \u001b[39m    Return the minimum of an array or minimum along an axis.\u001b[39;00m\n\u001b[0;32m   2747\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2856\u001b[0m \u001b[39m    6\u001b[39;00m\n\u001b[0;32m   2857\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2858\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapreduction(a, np\u001b[39m.\u001b[39;49mminimum, \u001b[39m'\u001b[39;49m\u001b[39mmin\u001b[39;49m\u001b[39m'\u001b[39;49m, axis, \u001b[39mNone\u001b[39;49;00m, out,\n\u001b[0;32m   2859\u001b[0m                           keepdims\u001b[39m=\u001b[39;49mkeepdims, initial\u001b[39m=\u001b[39;49minitial, where\u001b[39m=\u001b[39;49mwhere)\n",
      "File \u001b[1;32mc:\\Users\\anhad\\miniconda3\\envs\\ml\\lib\\site-packages\\numpy\\core\\fromnumeric.py:87\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[1;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[0;32m     84\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     85\u001b[0m             \u001b[39mreturn\u001b[39;00m reduction(axis\u001b[39m=\u001b[39maxis, out\u001b[39m=\u001b[39mout, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpasskwargs)\n\u001b[1;32m---> 87\u001b[0m \u001b[39mreturn\u001b[39;00m ufunc\u001b[39m.\u001b[39mreduce(obj, axis, dtype, out, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpasskwargs)\n",
      "\u001b[1;31mValueError\u001b[0m: zero-size array to reduction operation minimum which has no identity"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def imshow(img):\n",
    "  # Invalid shape (1, 224, 224)\n",
    "  # plt.imshow(img)\n",
    "  plt.imshow(np.transpose(img, (1, 2, 0)))\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for  images, label, votes in iter(loader):\n",
    "  images = images.numpy()\n",
    "  label = label.numpy()\n",
    "  for i in range(9):\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    imshow(images[i,...])\n",
    "    title = \"%d %f\" % (label[i]+1, votes[i], )\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
